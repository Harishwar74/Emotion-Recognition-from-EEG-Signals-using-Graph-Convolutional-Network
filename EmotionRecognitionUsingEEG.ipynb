{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wT9lS-EVRmW",
        "outputId": "13609648-3cb5-4fee-fdb3-e57c2ac21a98"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVxqtc9YVQND",
        "outputId": "a4b52e34-7684-4670-a1d3-2ff1ac0bf690"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Best Model Pipeline for TAR + GCN with Joint Improvements\n",
        "\n",
        "This code implements:\n",
        "  - A TAR module that uses both a supervised head and a contextual loss.\n",
        "  - A GCN module that leverages a Chebyshev convolution architecture.\n",
        "  - Separate training for TAR and GCN, with options for joint fine-tuning.\n",
        "\n",
        "Adjust the hyperparameters below as needed.\n",
        "\"\"\"\n",
        "\n",
        "import os, pickle\n",
        "import numpy as np\n",
        "import scipy.signal as signal\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_multi_labels(ratings, threshold=5):\n",
        "    \"\"\"Convert ratings into binary labels for [arousal, valence, dominance].\"\"\"\n",
        "    arousal   = 1 if ratings[0] > threshold else 0\n",
        "    valence   = 1 if ratings[1] > threshold else 0\n",
        "    dominance = 1 if ratings[2] > threshold else 0\n",
        "    return [arousal, valence, dominance]\n",
        "\n",
        "\n",
        "class DEAPMultiLabelDataset(Dataset):\n",
        "    def __init__(self, data_dir, subject_list, transform=None, threshold=5):\n",
        "        self.data = []   # (eeg_trial, multi-label vector)\n",
        "        self.transform = transform\n",
        "        self.threshold = threshold\n",
        "\n",
        "        for subj in subject_list:\n",
        "            fname = f\"s{subj:02d}.dat\"\n",
        "            file_path = os.path.join(data_dir, fname)\n",
        "            if not os.path.exists(file_path):\n",
        "                print(f\"Warning: {file_path} not found.\")\n",
        "                continue\n",
        "\n",
        "            with open(file_path, 'rb') as f:\n",
        "                subj_data = pickle.load(f, encoding='latin1')\n",
        "                eeg_trials = subj_data['data']    # shape: (trials, channels, signal_length)\n",
        "                labels     = subj_data['labels']    # shape: (trials, num_ratings)\n",
        "                for trial, lab in zip(eeg_trials, labels):\n",
        "                    multi_label = get_multi_labels(lab, threshold=self.threshold)\n",
        "                    self.data.append((trial, multi_label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        eeg_trial, label = self.data[idx]\n",
        "        if self.transform:\n",
        "            eeg_trial = self.transform(eeg_trial)\n",
        "        return eeg_trial, torch.tensor(label, dtype=torch.float)\n",
        "\n",
        "\n",
        "def welch_spectral_features(signal_data, fs=128, freq_band=(4,45), nperseg=128):\n",
        "    f, Pxx = signal.welch(signal_data, fs=fs, nperseg=nperseg)\n",
        "    idx = np.where((f >= freq_band[0]) & (f <= freq_band[1]))[0]\n",
        "    return Pxx[idx]  # 1-D vector\n",
        "\n",
        "\n",
        "def extract_spatial_spectral(eeg_trial, mapping_matrix, fs=128):\n",
        "    C = eeg_trial.shape[0]\n",
        "    spectral_features = []\n",
        "    for ch in range(C):\n",
        "        feat = welch_spectral_features(eeg_trial[ch, :], fs=fs)\n",
        "        spectral_features.append(feat)\n",
        "    spectral_features = np.array(spectral_features)  # shape: (C, B)\n",
        "\n",
        "    P, Q = mapping_matrix.shape\n",
        "    B = spectral_features.shape[1]\n",
        "    feature_map = np.zeros((P, Q, B), dtype=np.float32)\n",
        "    for i in range(P):\n",
        "        for j in range(Q):\n",
        "            ch_idx = mapping_matrix[i, j]\n",
        "            if 0 <= ch_idx < C:\n",
        "                feature_map[i, j, :] = spectral_features[ch_idx, :]\n",
        "    return feature_map\n",
        "\n",
        "\n",
        "class SpatialSpectralTransform:\n",
        "    def __init__(self, mapping_matrix, fs=128):\n",
        "        self.mapping_matrix = mapping_matrix\n",
        "        self.fs = fs\n",
        "\n",
        "    def __call__(self, eeg_trial):\n",
        "        feat = extract_spatial_spectral(eeg_trial, self.mapping_matrix, fs=self.fs)\n",
        "        # Rearrange to (B, P, Q) for CNN input.\n",
        "        feat = np.transpose(feat, (2, 0, 1))\n",
        "        return torch.from_numpy(feat)\n",
        "\n",
        "\n",
        "# Build mapping matrix for 32 channels in a 4 x 8 grid.\n",
        "P, Q = 4, 8\n",
        "mapping_matrix = -1 * np.ones((P, Q), dtype=np.int32)\n",
        "channels = list(range(32))\n",
        "for idx, ch in enumerate(channels):\n",
        "    i = idx // Q\n",
        "    j = idx % Q\n",
        "    mapping_matrix[i, j] = ch"
      ],
      "metadata": {
        "id": "oOqbwKe8O1ZV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 2. TAR_Transformer: CNN + Transformer with Supervised Head\n",
        "\n",
        "class TAR_Transformer(nn.Module):\n",
        "    def __init__(self, in_channels, out_features=128, num_heads=4, num_layers=2, num_classes=3):\n",
        "\n",
        "        super(TAR_Transformer, self).__init__()\n",
        "\n",
        "        # CNN part\n",
        "        self.conv1a = nn.Conv2d(in_channels, 42, kernel_size=2, stride=1)\n",
        "        self.conv1b = nn.Conv2d(in_channels, 42, kernel_size=2, stride=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=1)\n",
        "        self.conv2 = nn.Conv2d(42, 128, kernel_size=2, stride=2)\n",
        "\n",
        "        # Transformer encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=128, nhead=num_heads,\n",
        "            dim_feedforward=256, dropout=0.1, batch_first=True)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.fc = nn.Linear(128, out_features)\n",
        "        self.classifier = nn.Linear(out_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1a(x) + self.conv1b(x)\n",
        "        out = self.pool(out)\n",
        "        out = self.conv2(out)\n",
        "        out = F.relu(out)\n",
        "\n",
        "        batch_size, channels, p, q = out.shape\n",
        "        seq_len = p * q\n",
        "        out = out.permute(0, 2, 3, 1).reshape(batch_size, seq_len, 128)\n",
        "\n",
        "        out = self.transformer(out)\n",
        "        emb = self.fc(out.mean(dim=1))\n",
        "        logits = self.classifier(emb)\n",
        "        return emb, logits\n",
        "\n"
      ],
      "metadata": {
        "id": "kazsspDbO_mJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 3. Loss Functions for TAR & Graph Construction\n",
        "\n",
        "\n",
        "def compute_contextual_loss(features, h=0.1):\n",
        "    batch_size = features.size(0)\n",
        "    normed = F.normalize(features, p=2, dim=1)\n",
        "    sim_matrix = torch.matmul(normed, normed.t())\n",
        "    d_matrix = 1 - sim_matrix\n",
        "    d_matrix_no_diag = d_matrix + torch.eye(batch_size, device=features.device)*1e6\n",
        "    d_min, _ = torch.min(d_matrix_no_diag, dim=1, keepdim=True)\n",
        "    d_tilde = d_matrix / (d_min + 1e-5)\n",
        "    omega = torch.exp((1 - d_tilde) / h)\n",
        "    max_per_col, _ = torch.max(omega, dim=0, keepdim=True)\n",
        "    Z = omega / (max_per_col + 1e-5)\n",
        "    Lz = -torch.log(Z.mean() + 1e-5)\n",
        "    return Lz\n",
        "\n",
        "def compute_adjacency_loss(Lz, R_star, gamma=1.3):\n",
        "    diff = torch.abs(Lz - R_star)\n",
        "    return gamma ** diff\n",
        "\n"
      ],
      "metadata": {
        "id": "mrNUtViyPEOW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 4. GCN Module: Chebyshev-based Graph Convolution and Multi-Task GCN\n",
        "\n",
        "\n",
        "class ChebConv(nn.Module):\n",
        "    def __init__(self, in_features, out_features, K, lambda_max=2):\n",
        "        super(ChebConv, self).__init__()\n",
        "        self.K = K\n",
        "        self.theta = nn.Parameter(torch.Tensor(K+1, in_features, out_features))\n",
        "        nn.init.xavier_uniform_(self.theta)\n",
        "\n",
        "    def forward(self, X, L):\n",
        "        N = X.size(0)\n",
        "        L_tilde = (2.0 / 2) * L - torch.eye(N, device=X.device)  # lambda_max approximated as 2\n",
        "        T_k = [torch.eye(N, device=X.device), L_tilde]\n",
        "        for k in range(2, self.K+1):\n",
        "            T_k.append(2 * torch.mm(L_tilde, T_k[-1]) - T_k[-2])\n",
        "        out = 0\n",
        "        for k in range(self.K+1):\n",
        "            out += torch.mm(T_k[k], X) @ self.theta[k]\n",
        "        return out\n",
        "\n",
        "class GCN_MultiTask(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=128, out_features=3, K=3):\n",
        "        super(GCN_MultiTask, self).__init__()\n",
        "        self.conv1 = ChebConv(in_features, hidden_features, K)\n",
        "        self.conv2 = ChebConv(hidden_features, out_features, K)\n",
        "\n",
        "    def forward(self, X, L):\n",
        "        x = F.relu(self.conv1(X, L))\n",
        "        x = self.conv2(x, L)  # (N, 3) logits\n",
        "        return x\n",
        "\n",
        "def compute_normalized_laplacian(A):\n",
        "    d = torch.sum(A, dim=1)\n",
        "    D_inv_sqrt = torch.diag(1.0 / torch.sqrt(d + 1e-5))\n",
        "    L = torch.eye(A.size(0), device=A.device) - D_inv_sqrt @ A @ D_inv_sqrt\n",
        "    return L\n"
      ],
      "metadata": {
        "id": "QnsF79lzPLi1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 5. Training Functions for TAR and GCN\n",
        "\n",
        "\n",
        "def train_tar(tar_model, dataloader, num_epochs=20, lr=5e-4, alpha=0.6, beta=0.4):\n",
        "    \"\"\"\n",
        "    Trains TAR with a combined loss:\n",
        "      L_total = alpha * L_supervised + beta * L_contextual.\n",
        "    \"\"\"\n",
        "    tar_model.train()\n",
        "    optimizer = optim.Adam(tar_model.parameters(), lr=lr)\n",
        "    cls_loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs = inputs.to(device).float()\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            emb, logits = tar_model(inputs)\n",
        "            supervised_loss = cls_loss_fn(logits, labels)\n",
        "            contextual_loss = compute_contextual_loss(emb)\n",
        "            total_loss = alpha * supervised_loss + beta * contextual_loss\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += total_loss.item()\n",
        "        print(f\"TAR Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(dataloader):.4f}\")\n",
        "    return tar_model\n",
        "\n",
        "def predict_adjacency(tar_model, dataloader, h=0.1, threshold=0.5):\n",
        "    tar_model.eval()\n",
        "    emb_list = []\n",
        "    label_list = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs = inputs.to(device).float()\n",
        "            emb, _ = tar_model(inputs)\n",
        "            emb_list.append(emb)\n",
        "            label_list.append(labels.to(device))\n",
        "    features_all = torch.cat(emb_list, dim=0)\n",
        "    labels_all = torch.cat(label_list, dim=0)\n",
        "\n",
        "    normed = F.normalize(features_all, p=2, dim=1)\n",
        "    sim_matrix = torch.matmul(normed, normed.t())\n",
        "    d_matrix = 1 - sim_matrix\n",
        "    d_matrix_no_diag = d_matrix + torch.eye(d_matrix.size(0), device=device)*1e6\n",
        "    d_min, _ = torch.min(d_matrix_no_diag, dim=1, keepdim=True)\n",
        "    d_tilde = d_matrix / (d_min + 1e-5)\n",
        "    omega = torch.exp((1 - d_tilde)/h)\n",
        "    max_per_col, _ = torch.max(omega, dim=0, keepdim=True)\n",
        "    Z = omega / (max_per_col + 1e-5)\n",
        "    A_pred = (Z > np.exp((1 - threshold)/h)).float()\n",
        "\n",
        "\n",
        "    plt.title(\"Predicted Adjacency Matrix\")\n",
        "\n",
        "\n",
        "    return A_pred.to(device), labels_all, features_all\n",
        "\n",
        "def train_gcn_multitask(gcn_model, features, A, labels, num_epochs=200, lr=2e-4, weight_decay=1e-4,model_path=\"/content/drive/My Drive/models/tardgcn_model13.pth\"):\n",
        "    gcn_model.train()\n",
        "    optimizer = optim.Adam(gcn_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20, verbose=True)\n",
        "    L = compute_normalized_laplacian(A)\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        optimizer.zero_grad()\n",
        "        output = gcn_model(features, L)\n",
        "        loss = loss_fn(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step(loss)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            preds = (torch.sigmoid(output) > 0.5).float()\n",
        "            acc = (preds == labels).float().mean().item() * 100\n",
        "            logits_mean = output.mean().item()\n",
        "            logits_std = output.std().item()\n",
        "        if (epoch+1) % 10 == 0:\n",
        "            print(f\"GCN Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}, Accuracy: {acc:.2f}%\")\n",
        "            # print(f\"  Logits: mean={logits_mean:.4f}, std={logits_std:.4f}\")\n",
        "    torch.save(gcn_model.state_dict(), model_path)\n",
        "    print(f\"Model saved to Google Drive at {model_path}\")\n",
        "    return gcn_model"
      ],
      "metadata": {
        "id": "P5W7DDDhPR9X"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 6. Main Execution: Data Loading, Training, and Testing\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    data_dir = '/content/drive/MyDrive/deap_dataset'\n",
        "    train_subjects = list(range(1, 26))\n",
        "    test_subjects  = list(range(26, 33))\n",
        "\n",
        "    transform = SpatialSpectralTransform(mapping_matrix, fs=128)\n",
        "    train_dataset = DEAPMultiLabelDataset(data_dir=data_dir, subject_list=train_subjects, transform=transform)\n",
        "    test_dataset  = DEAPMultiLabelDataset(data_dir=data_dir, subject_list=test_subjects, transform=transform)\n",
        "\n",
        "    batch_size = 16\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Initialize TAR module.\n",
        "    sample_eeg, _ = train_dataset[0]\n",
        "    in_channels = sample_eeg.shape[0]\n",
        "    tar_model = TAR_Transformer(in_channels=in_channels, out_features=128, num_heads=4, num_layers=2, num_classes=3).to(device)\n",
        "\n",
        "    print(\"Starting TAR training with combined loss...\")\n",
        "    tar_model = train_tar(tar_model, train_loader, num_epochs=10, lr=5e-4, alpha=0.6, beta=0.4)\n",
        "\n",
        "    print(\"Predicting adjacency using TAR features...\")\n",
        "    A_hat, train_labels, train_features = predict_adjacency(tar_model, train_loader)\n",
        "\n",
        "    # Train GCN with improved settings.\n",
        "    gcn_model = GCN_MultiTask(in_features=128, hidden_features=128, out_features=3, K=3).to(device)\n",
        "    print(\"Starting GCN training...\")\n",
        "    gcn_model = train_gcn_multitask(gcn_model, train_features, A_hat, train_labels, num_epochs=200, lr=2e-4, weight_decay=1e-4,model_path=\"/content/drive/My Drive/models/tardgcn_model13.pth\")\n",
        "\n",
        "    # Testing Phase.\n",
        "    tar_model.eval()\n",
        "    test_emb_list = []\n",
        "    test_label_list = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs = inputs.to(device).float()\n",
        "            emb, _ = tar_model(inputs)\n",
        "            test_emb_list.append(emb)\n",
        "            test_label_list.append(labels.to(device))\n",
        "    test_features = torch.cat(test_emb_list, dim=0)\n",
        "    test_labels = torch.cat(test_label_list, dim=0)\n",
        "\n",
        "    N_test = test_features.size(0)\n",
        "    L_test = torch.eye(N_test, device=device)\n",
        "\n",
        "    gcn_model.eval()\n",
        "    with torch.no_grad():\n",
        "        output_test = gcn_model(test_features, L_test)\n",
        "        probs = torch.sigmoid(output_test)\n",
        "        preds = (probs > 0.5).float()\n",
        "\n",
        "    correct = (preds == test_labels).float()\n",
        "    acc_per_dim = correct.mean(dim=0) * 100\n",
        "    print(\"Test Accuracy (in %):\")\n",
        "    print(f\"  Arousal: {acc_per_dim[0].item():.2f}%\")\n",
        "    print(f\"  Valence: {acc_per_dim[1].item():.2f}%\")\n",
        "    print(f\"  Dominance: {acc_per_dim[2].item():.2f}%\")\n",
        "\n",
        "\n",
        "    # (Optional) Joint Fine-Tuning of TAR and GCN\n",
        "\n",
        "    joint_params = list(tar_model.parameters()) + list(gcn_model.parameters())\n",
        "    joint_optimizer = optim.Adam(joint_params, lr=1e-4, weight_decay=1e-4)\n",
        "    num_joint_epochs = 20\n",
        "    for epoch in range(num_joint_epochs):\n",
        "        tar_model.train()\n",
        "        gcn_model.train()\n",
        "        joint_optimizer.zero_grad()\n",
        "        emb_list, label_list = [], []\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device).float()\n",
        "            labels = labels.to(device)\n",
        "            emb, _ = tar_model(inputs)\n",
        "            emb_list.append(emb)\n",
        "            label_list.append(labels)\n",
        "        features_joint = torch.cat(emb_list, dim=0)\n",
        "        labels_joint = torch.cat(label_list, dim=0)\n",
        "        A_joint, _, _ = predict_adjacency(tar_model, train_loader)\n",
        "        L_joint = compute_normalized_laplacian(A_joint)\n",
        "        output_joint = gcn_model(features_joint, L_joint)\n",
        "        joint_loss = nn.BCEWithLogitsLoss()(output_joint, labels_joint)\n",
        "        joint_loss.backward()\n",
        "        joint_optimizer.step()\n",
        "        if (epoch+1) % 10 == 0:\n",
        "            preds_joint = (torch.sigmoid(output_joint) > 0.5).float()\n",
        "            acc_joint = (preds_joint == labels_joint).float().mean().item() * 100\n",
        "            print(f\"Joint Epoch {epoch+1}/{num_joint_epochs}, Loss: {joint_loss.item():.4f}, Accuracy: {acc_joint:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "R5tVAyC_PbGy",
        "outputId": "91e012dd-fb29-403e-a3ce-a671aaac8e18"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting TAR training with combined loss...\n",
            "TAR Epoch 1/10, Loss: 1.4602\n",
            "TAR Epoch 2/10, Loss: 1.4534\n",
            "TAR Epoch 3/10, Loss: 1.4481\n",
            "TAR Epoch 4/10, Loss: 1.4432\n",
            "TAR Epoch 5/10, Loss: 1.4427\n",
            "TAR Epoch 6/10, Loss: 1.4417\n",
            "TAR Epoch 7/10, Loss: 1.4388\n",
            "TAR Epoch 8/10, Loss: 1.4389\n",
            "TAR Epoch 9/10, Loss: 1.4426\n",
            "TAR Epoch 10/10, Loss: 1.4358\n",
            "Predicting adjacency using TAR features...\n",
            "Starting GCN training...\n",
            "GCN Epoch 10/200, Loss: 0.6186, Accuracy: 70.50%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCN Epoch 20/200, Loss: 0.5681, Accuracy: 71.13%\n",
            "GCN Epoch 30/200, Loss: 0.5448, Accuracy: 72.93%\n",
            "GCN Epoch 40/200, Loss: 0.5369, Accuracy: 73.33%\n",
            "GCN Epoch 50/200, Loss: 0.5338, Accuracy: 73.53%\n",
            "GCN Epoch 60/200, Loss: 0.5323, Accuracy: 73.67%\n",
            "GCN Epoch 70/200, Loss: 0.5313, Accuracy: 73.70%\n",
            "GCN Epoch 80/200, Loss: 0.5305, Accuracy: 73.73%\n",
            "GCN Epoch 90/200, Loss: 0.5298, Accuracy: 73.87%\n",
            "GCN Epoch 100/200, Loss: 0.5292, Accuracy: 73.97%\n",
            "GCN Epoch 110/200, Loss: 0.5287, Accuracy: 73.93%\n",
            "GCN Epoch 120/200, Loss: 0.5282, Accuracy: 74.10%\n",
            "GCN Epoch 130/200, Loss: 0.5276, Accuracy: 74.20%\n",
            "GCN Epoch 140/200, Loss: 0.5271, Accuracy: 74.23%\n",
            "GCN Epoch 150/200, Loss: 0.5266, Accuracy: 74.27%\n",
            "GCN Epoch 160/200, Loss: 0.5260, Accuracy: 74.30%\n",
            "GCN Epoch 170/200, Loss: 0.5255, Accuracy: 74.40%\n",
            "GCN Epoch 180/200, Loss: 0.5249, Accuracy: 74.40%\n",
            "GCN Epoch 190/200, Loss: 0.5243, Accuracy: 74.53%\n",
            "GCN Epoch 200/200, Loss: 0.5237, Accuracy: 74.53%\n",
            "Model saved to Google Drive at /content/drive/My Drive/models/tardgcn_model13.pth\n",
            "Test Accuracy (in %):\n",
            "  Arousal: 76.07%\n",
            "  Valence: 75.36%\n",
            "  Dominance: 57.86%\n",
            "Joint Epoch 10/20, Loss: 0.5215, Accuracy: 74.07%\n",
            "Joint Epoch 20/20, Loss: 0.5140, Accuracy: 74.73%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def map_emotion(predicted_label):\n",
        "\n",
        "    if isinstance(predicted_label, torch.Tensor):\n",
        "        predicted_label = predicted_label.tolist()\n",
        "\n",
        "    arousal, valence, dominance = int(predicted_label[0]), int(predicted_label[1]), int(predicted_label[2])\n",
        "\n",
        "    if arousal == 1 and valence == 1:\n",
        "        emotion = \"Happy/Excited\"\n",
        "    elif arousal == 1 and valence == 0:\n",
        "        emotion = \"Angry/Agitated\"\n",
        "    elif arousal == 0 and valence == 1:\n",
        "        emotion = \"Calm/Relaxed\"\n",
        "    else:  # arousal == 0 and valence == 0\n",
        "        emotion = \"Sad/Melancholic\"\n",
        "\n",
        "    if dominance == 1:\n",
        "        dominance_desc = \"with confidence\"\n",
        "    else:\n",
        "        dominance_desc = \"with less control\"\n",
        "\n",
        "    return f\"{emotion} {dominance_desc}\"\n"
      ],
      "metadata": {
        "id": "vYX2OdCchzNO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "gcn_model_path = \"/content/drive/My Drive/models/tardgcn_model13.pth\"\n",
        "\n",
        "gcn_model = GCN_MultiTask(in_features=128, hidden_features=128, out_features=3, K=3).to(device)\n",
        "\n",
        "gcn_model.load_state_dict(torch.load(gcn_model_path, map_location=device))\n",
        "gcn_model.eval()\n",
        "\n",
        "\n",
        "# Prediction Function Using Loaded GCN Model\n",
        "\n",
        "def predict_sample(tar_model, gcn_model, dataset, index, device):\n",
        "\n",
        "\n",
        "    tar_model.eval()\n",
        "    gcn_model.eval()\n",
        "\n",
        "    sample, original_label = dataset[index]\n",
        "\n",
        "    sample = sample.unsqueeze(0).to(device).float()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        emb, _ = tar_model(sample)\n",
        "        L_identity = torch.eye(emb.size(0), device=device)\n",
        "        gcn_output = gcn_model(emb, L_identity)\n",
        "        probs = torch.sigmoid(gcn_output)\n",
        "        pred_label = (probs > 0.5).float().squeeze(0)\n",
        "\n",
        "    return original_label, pred_label\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXlw9tSdqs_q",
        "outputId": "0be4cb98-b444-4cf3-9158-08d5affe5884"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-a5f0a91e0a4c>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  gcn_model.load_state_dict(torch.load(gcn_model_path, map_location=device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Prediction on Individual Test Samples ---\n",
            "\n",
            "Sample index 201:\n",
            "  Original Label : [0.0, 0.0, 1.0]\n",
            "  Predicted Label: [0.0, 0.0, 1.0]\n",
            "\n",
            "Original Mapped Emotion : Sad/Melancholic with confidence\n",
            "Predicted Mapped Emotion: Sad/Melancholic with confidence\n",
            "\n",
            "\n",
            "Sample index 202:\n",
            "  Original Label : [0.0, 0.0, 0.0]\n",
            "  Predicted Label: [0.0, 0.0, 1.0]\n",
            "\n",
            "Original Mapped Emotion : Sad/Melancholic with less control\n",
            "Predicted Mapped Emotion: Sad/Melancholic with confidence\n",
            "\n",
            "\n",
            "Sample index 203:\n",
            "  Original Label : [0.0, 0.0, 1.0]\n",
            "  Predicted Label: [0.0, 0.0, 1.0]\n",
            "\n",
            "Original Mapped Emotion : Sad/Melancholic with confidence\n",
            "Predicted Mapped Emotion: Sad/Melancholic with confidence\n",
            "\n",
            "\n",
            "Sample index 204:\n",
            "  Original Label : [1.0, 0.0, 1.0]\n",
            "  Predicted Label: [0.0, 0.0, 1.0]\n",
            "\n",
            "Original Mapped Emotion : Angry/Agitated with confidence\n",
            "Predicted Mapped Emotion: Sad/Melancholic with confidence\n",
            "\n",
            "\n",
            "Sample index 205:\n",
            "  Original Label : [0.0, 0.0, 1.0]\n",
            "  Predicted Label: [0.0, 0.0, 1.0]\n",
            "\n",
            "Original Mapped Emotion : Sad/Melancholic with confidence\n",
            "Predicted Mapped Emotion: Sad/Melancholic with confidence\n",
            "\n",
            "\n",
            "Sample index 206:\n",
            "  Original Label : [0.0, 0.0, 1.0]\n",
            "  Predicted Label: [0.0, 0.0, 1.0]\n",
            "\n",
            "Original Mapped Emotion : Sad/Melancholic with confidence\n",
            "Predicted Mapped Emotion: Sad/Melancholic with confidence\n",
            "\n",
            "\n",
            "Sample index 207:\n",
            "  Original Label : [0.0, 0.0, 1.0]\n",
            "  Predicted Label: [0.0, 0.0, 1.0]\n",
            "\n",
            "Original Mapped Emotion : Sad/Melancholic with confidence\n",
            "Predicted Mapped Emotion: Sad/Melancholic with confidence\n",
            "\n",
            "\n",
            "Sample index 208:\n",
            "  Original Label : [1.0, 0.0, 1.0]\n",
            "  Predicted Label: [0.0, 0.0, 1.0]\n",
            "\n",
            "Original Mapped Emotion : Angry/Agitated with confidence\n",
            "Predicted Mapped Emotion: Sad/Melancholic with confidence\n",
            "\n",
            "\n",
            "Sample index 209:\n",
            "  Original Label : [1.0, 0.0, 1.0]\n",
            "  Predicted Label: [0.0, 0.0, 1.0]\n",
            "\n",
            "Original Mapped Emotion : Angry/Agitated with confidence\n",
            "Predicted Mapped Emotion: Sad/Melancholic with confidence\n",
            "\n",
            "\n",
            "Sample index 210:\n",
            "  Original Label : [0.0, 1.0, 0.0]\n",
            "  Predicted Label: [0.0, 0.0, 0.0]\n",
            "\n",
            "Original Mapped Emotion : Calm/Relaxed with less control\n",
            "Predicted Mapped Emotion: Sad/Melancholic with less control\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict and Print Outputs for Samples\n",
        "\n",
        "print(\"\\n--- Prediction on Individual Test Samples ---\\n\")\n",
        "num_samples_to_predict = 5\n",
        "for i in range(45,75):\n",
        "    orig_label, prediction = predict_sample(tar_model, gcn_model, test_dataset, i, device)\n",
        "    orig_list = orig_label.tolist()\n",
        "    pred_list = prediction.tolist()\n",
        "    print(f\"Sample index {i+1}:\")\n",
        "    print(f\"  Original Label : {orig_list}\")\n",
        "    print(f\"  Predicted Label: {pred_list}\\n\")\n",
        "    print(\"Original Mapped Emotion :\", map_emotion(orig_list))\n",
        "    print(\"Predicted Mapped Emotion:\", map_emotion(pred_list))\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqwOFVQKX_Wo",
        "outputId": "ad454037-0c2d-45e4-fd5c-3b2016424094"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Prediction on Individual Test Samples ---\n",
            "\n",
            "Sample index 46:\n",
            "  Original Label : [0.0, 0.0, 1.0]\n",
            "  Predicted Label: [0.0, 0.0, 1.0]\n",
            "\n",
            "Original Mapped Emotion : Sad/Melancholic with confidence\n",
            "Predicted Mapped Emotion: Sad/Melancholic with confidence\n",
            "\n",
            "\n",
            "Sample index 47:\n",
            "  Original Label : [0.0, 1.0, 1.0]\n",
            "  Predicted Label: [0.0, 0.0, 1.0]\n",
            "\n",
            "Original Mapped Emotion : Calm/Relaxed with confidence\n",
            "Predicted Mapped Emotion: Sad/Melancholic with confidence\n",
            "\n",
            "\n",
            "Sample index 48:\n",
            "  Original Label : [0.0, 0.0, 1.0]\n",
            "  Predicted Label: [0.0, 0.0, 0.0]\n",
            "\n",
            "Original Mapped Emotion : Sad/Melancholic with confidence\n",
            "Predicted Mapped Emotion: Sad/Melancholic with less control\n",
            "\n",
            "\n",
            "Sample index 49:\n",
            "  Original Label : [0.0, 0.0, 1.0]\n",
            "  Predicted Label: [0.0, 0.0, 1.0]\n",
            "\n",
            "Original Mapped Emotion : Sad/Melancholic with confidence\n",
            "Predicted Mapped Emotion: Sad/Melancholic with confidence\n",
            "\n",
            "\n",
            "Sample index 50:\n",
            "  Original Label : [0.0, 0.0, 1.0]\n",
            "  Predicted Label: [0.0, 0.0, 0.0]\n",
            "\n",
            "Original Mapped Emotion : Sad/Melancholic with confidence\n",
            "Predicted Mapped Emotion: Sad/Melancholic with less control\n",
            "\n",
            "\n",
            "Sample index 51:\n",
            "  Original Label : [0.0, 0.0, 1.0]\n",
            "  Predicted Label: [0.0, 0.0, 1.0]\n",
            "\n",
            "Original Mapped Emotion : Sad/Melancholic with confidence\n",
            "Predicted Mapped Emotion: Sad/Melancholic with confidence\n",
            "\n",
            "\n",
            "Sample index 52:\n",
            "  Original Label : [0.0, 1.0, 1.0]\n",
            "  Predicted Label: [0.0, 0.0, 0.0]\n",
            "\n",
            "Original Mapped Emotion : Calm/Relaxed with confidence\n",
            "Predicted Mapped Emotion: Sad/Melancholic with less control\n",
            "\n",
            "\n",
            "Sample index 53:\n",
            "  Original Label : [0.0, 0.0, 1.0]\n",
            "  Predicted Label: [0.0, 0.0, 1.0]\n",
            "\n",
            "Original Mapped Emotion : Sad/Melancholic with confidence\n",
            "Predicted Mapped Emotion: Sad/Melancholic with confidence\n",
            "\n",
            "\n",
            "Sample index 54:\n",
            "  Original Label : [0.0, 0.0, 1.0]\n",
            "  Predicted Label: [0.0, 0.0, 0.0]\n",
            "\n",
            "Original Mapped Emotion : Sad/Melancholic with confidence\n",
            "Predicted Mapped Emotion: Sad/Melancholic with less control\n",
            "\n",
            "\n",
            "Sample index 55:\n",
            "  Original Label : [1.0, 0.0, 1.0]\n",
            "  Predicted Label: [0.0, 0.0, 1.0]\n",
            "\n",
            "Original Mapped Emotion : Angry/Agitated with confidence\n",
            "Predicted Mapped Emotion: Sad/Melancholic with confidence\n",
            "\n",
            "\n",
            "Sample index 56:\n",
            "  Original Label : [0.0, 0.0, 1.0]\n",
            "  Predicted Label: [0.0, 0.0, 0.0]\n",
            "\n",
            "Original Mapped Emotion : Sad/Melancholic with confidence\n",
            "Predicted Mapped Emotion: Sad/Melancholic with less control\n",
            "\n",
            "\n",
            "Sample index 57:\n",
            "  Original Label : [0.0, 0.0, 1.0]\n",
            "  Predicted Label: [0.0, 0.0, 0.0]\n",
            "\n",
            "Original Mapped Emotion : Sad/Melancholic with confidence\n",
            "Predicted Mapped Emotion: Sad/Melancholic with less control\n",
            "\n",
            "\n",
            "Sample index 58:\n",
            "  Original Label : [0.0, 0.0, 1.0]\n",
            "  Predicted Label: [0.0, 0.0, 1.0]\n",
            "\n",
            "Original Mapped Emotion : Sad/Melancholic with confidence\n",
            "Predicted Mapped Emotion: Sad/Melancholic with confidence\n",
            "\n",
            "\n",
            "Sample index 59:\n",
            "  Original Label : [1.0, 0.0, 1.0]\n",
            "  Predicted Label: [0.0, 0.0, 1.0]\n",
            "\n",
            "Original Mapped Emotion : Angry/Agitated with confidence\n",
            "Predicted Mapped Emotion: Sad/Melancholic with confidence\n",
            "\n",
            "\n",
            "Sample index 60:\n",
            "  Original Label : [0.0, 0.0, 1.0]\n",
            "  Predicted Label: [0.0, 0.0, 1.0]\n",
            "\n",
            "Original Mapped Emotion : Sad/Melancholic with confidence\n",
            "Predicted Mapped Emotion: Sad/Melancholic with confidence\n",
            "\n",
            "\n",
            "Sample index 61:\n",
            "  Original Label : [0.0, 0.0, 1.0]\n",
            "  Predicted Label: [0.0, 0.0, 1.0]\n",
            "\n",
            "Original Mapped Emotion : Sad/Melancholic with confidence\n",
            "Predicted Mapped Emotion: Sad/Melancholic with confidence\n",
            "\n",
            "\n",
            "Sample index 62:\n",
            "  Original Label : [0.0, 0.0, 1.0]\n",
            "  Predicted Label: [0.0, 0.0, 0.0]\n",
            "\n",
            "Original Mapped Emotion : Sad/Melancholic with confidence\n",
            "Predicted Mapped Emotion: Sad/Melancholic with less control\n",
            "\n",
            "\n",
            "Sample index 63:\n",
            "  Original Label : [0.0, 0.0, 1.0]\n",
            "  Predicted Label: [0.0, 0.0, 1.0]\n",
            "\n",
            "Original Mapped Emotion : Sad/Melancholic with confidence\n",
            "Predicted Mapped Emotion: Sad/Melancholic with confidence\n",
            "\n",
            "\n",
            "Sample index 64:\n",
            "  Original Label : [0.0, 0.0, 1.0]\n",
            "  Predicted Label: [0.0, 0.0, 1.0]\n",
            "\n",
            "Original Mapped Emotion : Sad/Melancholic with confidence\n",
            "Predicted Mapped Emotion: Sad/Melancholic with confidence\n",
            "\n",
            "\n",
            "Sample index 65:\n",
            "  Original Label : [0.0, 0.0, 1.0]\n",
            "  Predicted Label: [0.0, 0.0, 1.0]\n",
            "\n",
            "Original Mapped Emotion : Sad/Melancholic with confidence\n",
            "Predicted Mapped Emotion: Sad/Melancholic with confidence\n",
            "\n",
            "\n",
            "Sample index 66:\n",
            "  Original Label : [1.0, 0.0, 1.0]\n",
            "  Predicted Label: [0.0, 0.0, 1.0]\n",
            "\n",
            "Original Mapped Emotion : Angry/Agitated with confidence\n",
            "Predicted Mapped Emotion: Sad/Melancholic with confidence\n",
            "\n",
            "\n",
            "Sample index 67:\n",
            "  Original Label : [0.0, 0.0, 1.0]\n",
            "  Predicted Label: [0.0, 0.0, 1.0]\n",
            "\n",
            "Original Mapped Emotion : Sad/Melancholic with confidence\n",
            "Predicted Mapped Emotion: Sad/Melancholic with confidence\n",
            "\n",
            "\n",
            "Sample index 68:\n",
            "  Original Label : [1.0, 0.0, 1.0]\n",
            "  Predicted Label: [0.0, 0.0, 0.0]\n",
            "\n",
            "Original Mapped Emotion : Angry/Agitated with confidence\n",
            "Predicted Mapped Emotion: Sad/Melancholic with less control\n",
            "\n",
            "\n",
            "Sample index 69:\n",
            "  Original Label : [0.0, 0.0, 1.0]\n",
            "  Predicted Label: [0.0, 0.0, 1.0]\n",
            "\n",
            "Original Mapped Emotion : Sad/Melancholic with confidence\n",
            "Predicted Mapped Emotion: Sad/Melancholic with confidence\n",
            "\n",
            "\n",
            "Sample index 70:\n",
            "  Original Label : [0.0, 0.0, 1.0]\n",
            "  Predicted Label: [0.0, 0.0, 0.0]\n",
            "\n",
            "Original Mapped Emotion : Sad/Melancholic with confidence\n",
            "Predicted Mapped Emotion: Sad/Melancholic with less control\n",
            "\n",
            "\n",
            "Sample index 71:\n",
            "  Original Label : [0.0, 0.0, 1.0]\n",
            "  Predicted Label: [0.0, 0.0, 1.0]\n",
            "\n",
            "Original Mapped Emotion : Sad/Melancholic with confidence\n",
            "Predicted Mapped Emotion: Sad/Melancholic with confidence\n",
            "\n",
            "\n",
            "Sample index 72:\n",
            "  Original Label : [0.0, 0.0, 1.0]\n",
            "  Predicted Label: [0.0, 0.0, 0.0]\n",
            "\n",
            "Original Mapped Emotion : Sad/Melancholic with confidence\n",
            "Predicted Mapped Emotion: Sad/Melancholic with less control\n",
            "\n",
            "\n",
            "Sample index 73:\n",
            "  Original Label : [1.0, 0.0, 1.0]\n",
            "  Predicted Label: [0.0, 0.0, 1.0]\n",
            "\n",
            "Original Mapped Emotion : Angry/Agitated with confidence\n",
            "Predicted Mapped Emotion: Sad/Melancholic with confidence\n",
            "\n",
            "\n",
            "Sample index 74:\n",
            "  Original Label : [0.0, 0.0, 1.0]\n",
            "  Predicted Label: [0.0, 0.0, 1.0]\n",
            "\n",
            "Original Mapped Emotion : Sad/Melancholic with confidence\n",
            "Predicted Mapped Emotion: Sad/Melancholic with confidence\n",
            "\n",
            "\n",
            "Sample index 75:\n",
            "  Original Label : [0.0, 0.0, 1.0]\n",
            "  Predicted Label: [0.0, 0.0, 1.0]\n",
            "\n",
            "Original Mapped Emotion : Sad/Melancholic with confidence\n",
            "Predicted Mapped Emotion: Sad/Melancholic with confidence\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}